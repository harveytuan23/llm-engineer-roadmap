{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31328d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duan/llm-engineer-roadmap/.llmvenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pypdf import PdfReader\n",
    "import faiss, numpy as np\n",
    "from groq import Groq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9b540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d76d8978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀 PDF\n",
    "pdf = PdfReader(\"data/sample.pdf\")\n",
    "raw = \"\\n\".join([p.extract_text() for p in pdf.pages if p.extract_text()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96cbd5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切 chunk\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "chunks = splitter.split_text(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb272fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "emb = embedder.encode(chunks, normalize_embeddings=True)\n",
    "emb = np.array(emb, dtype=\"float32\")\n",
    "dim = emb.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c578992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 index\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9545f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, k=5):\n",
    "    qv = embedder.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "    D, I = index.search(qv, k)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "\n",
    "def rag(question, k=5):\n",
    "    ctx = \"\\n\".join(search(question, k))\n",
    "    messages = [\n",
    "        {\"role\":\"system\",\"content\":\"You are a precise assistant. Answer ONLY using CONTEXT.\"},\n",
    "        {\"role\":\"user\",\"content\":f\"CONTEXT:\\n{ctx}\\n\\nQUESTION: {question}\"}\n",
    "    ]\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        max_tokens=400\n",
    "    )\n",
    "    return resp.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24fa6f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The methodology used in the paper involves the following:\n",
      "\n",
      "1. Data Collection and Preprocessing (Section 3.1), specifically setting up and composing data (Section 3.1.1)\n",
      "2. Using Deep Learning for Sign Language Recognition (Section 2.2), including CNN-based Methods (Section 2.2.1)\n",
      "3. Augmented model with superior generalization capabilities, demonstrated by uniform excellence across all metrics\n"
     ]
    }
   ],
   "source": [
    "print(rag(\"What methodology does the paper use?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd86f35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, max_turns=5):\n",
    "        self.turns = []\n",
    "        self.max_turns = max_turns\n",
    "    def add(self, q, a):\n",
    "        self.turns.append((q,a))\n",
    "        if len(self.turns) > self.max_turns:\n",
    "            self.turns.pop(0)\n",
    "    def text(self):\n",
    "        return \"\\n\".join([f\"User:{q}\\nAI:{a}\" for q,a in self.turns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d033b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory()\n",
    "\n",
    "def chat(question):\n",
    "    history = memory.text()\n",
    "    ctx = \"\\n\".join(search(question, 5))\n",
    "    messages = [\n",
    "        {\"role\":\"system\",\"content\":\"You are a precise assistant. Use CONTEXT and HISTORY.\"},\n",
    "        {\"role\":\"user\",\"content\":f\"HISTORY:\\n{history}\\n\\nCONTEXT:\\n{ctx}\\n\\nQUESTION:{question}\"}\n",
    "    ]\n",
    "    resp = client.chat.completions.create(model=\"llama-3.1-8b-instant\",messages=messages,temperature=0.0,max_tokens=400)\n",
    "    answer = resp.choices[0].message.content\n",
    "    memory.add(question, answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "740bdb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided CONTEXT, the paper uses the following methodology:\n",
      "\n",
      "1. Data Collection and Preprocessing (Section 3.1)\n",
      "   - Data Setup and Composition (Section 3.1.1)\n",
      "\n",
      "Additionally, it is mentioned that the paper uses the following approaches for Sign Language Recognition:\n",
      "\n",
      "1. Deep Learning (Section 2.2)\n",
      "   - CNN-based Methods (Section 2.2.1)\n",
      "\n",
      "No specific information on Model Architecture (Section 3.5) is provided in the given context.\n"
     ]
    }
   ],
   "source": [
    "print(chat(\"What methodology does the paper use?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5151f456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided CONTEXT, the results of the paper can be summarized as follows:\n",
      "\n",
      "1. **Effect of Data Augmentation**: The results show that the data augmentation techniques had a substantial impact on model performance. The model trained with augmented data achieved near-perfect accuracy (99.94%) on the validation set, compared to 80.17% for the non-augmented model.\n",
      "\n",
      "2. **Model Stability**: The augmented model demonstrated remarkable stability throughout training, with consistent performance and minimal fluctuations. In contrast, the non-augmented model exhibited pronounced instability throughout training.\n",
      "\n",
      "3. **Generalization Capabilities**: The results indicate that the augmented model has superior generalization capabilities, as demonstrated by its uniform excellence across all metrics.\n",
      "\n",
      "4. **Improved Accuracy**: The data augmentation techniques led to a significant improvement in model performance, with the augmented model achieving:\n",
      "   - 99.94% accuracy on the validation set\n",
      "   - Stability throughout training with consistent performance and minimal fluctuations\n",
      "\n",
      "By using the data augmentation techniques, the model was able to learn and generalize more effectively, leading to improved accuracy and stability.\n"
     ]
    }
   ],
   "source": [
    "print(chat(\"And what are the results?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
